{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PorterMathews/VectorDB_Lab/blob/main/embed/VectorDB_Lab_CS452_(starter).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UOUNsUTsvcn"
      },
      "outputs": [],
      "source": [
        "# Download datasets from kaggle\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "if not os.path.exists(\"lex-fridman-text-embedding-3-large-128.zip\"):\n",
        "  kaggle_json = {\"username\": \"michaeltreynolds\",\"key\": \"149701be742f30a8a0526762c61beea0\"}\n",
        "  kaggle_dir = os.path.join(os.path.expanduser(\"~\"), \".kaggle\")\n",
        "  os.makedirs(kaggle_dir, exist_ok=True)\n",
        "  kaggle_config_path = os.path.join(kaggle_dir, \"kaggle.json\")\n",
        "  with open(kaggle_config_path, 'w') as f:\n",
        "    json.dump(kaggle_json, f)\n",
        "\n",
        "  !kaggle datasets download -d michaeltreynolds/lex-fridman-text-embedding-3-large-128\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip kaggle data\n",
        "\n",
        "!unzip lex-fridman-text-embedding-3-large-128.zip\n",
        "!unzip lex-fridman-text-embedding-3-large-128/*.zip\n"
      ],
      "metadata": {
        "id": "h3swnD70x4FG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use specific libraries\n",
        "!pip install datasets==2.20.0 psycopg2==2.9.9 pgcopy==1.6.0\n",
        "import psycopg2"
      ],
      "metadata": {
        "id": "SYDFzWfv4HLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get your own trial account at timescaledb and paste your own connection string\n",
        "\n",
        "#TODO\n",
        "CONNECTION = \"REMOVED\""
      ],
      "metadata": {
        "id": "ukT4dY-z25XG"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use this if you want to start over on your postgres table!\n",
        "\n",
        "DROP_TABLE = \"DROP TABLE IF EXISTS podcast, segment\"\n",
        "with psycopg2.connect(CONNECTION) as conn:\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(DROP_TABLE)\n",
        "    conn.commit() # Commit the changes\n"
      ],
      "metadata": {
        "id": "gpp_3EuU3SN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Useful function that takes a pd.DataFrame and copies it directly into a table.\n",
        "\n",
        "import pandas as pd\n",
        "import io\n",
        "import psycopg2\n",
        "\n",
        "from typing import List\n",
        "\n",
        "def fast_pg_insert(df: pd.DataFrame, connection: str, table_name: str, columns: List[str]) -> None:\n",
        "    \"\"\"\n",
        "        Inserts data from a pandas DataFrame into a PostgreSQL table using the COPY command for fast insertion.\n",
        "\n",
        "        Parameters:\n",
        "        df (pd.DataFrame): The DataFrame containing the data to be inserted.\n",
        "        connection (str): The connection string to the PostgreSQL database.\n",
        "        table_name (str): The name of the target table in the PostgreSQL database.\n",
        "        columns (List[str]): A list of column names in the target table that correspond to the DataFrame columns.\n",
        "\n",
        "        Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    conn = psycopg2.connect(connection)\n",
        "    _buffer = io.StringIO()\n",
        "    df.to_csv(_buffer, sep=\";\", index=False, header=False)\n",
        "    _buffer.seek(0)\n",
        "    with conn.cursor() as c:\n",
        "        c.copy_from(\n",
        "            file=_buffer,\n",
        "            table=table_name,\n",
        "            sep=\";\",\n",
        "            columns=columns,\n",
        "            null=''\n",
        "        )\n",
        "    conn.commit()\n",
        "    conn.close()"
      ],
      "metadata": {
        "id": "wZDxdvoP4Fov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Database Schema\n",
        "We will create a database with two tables: podcast and segment:\n",
        "\n",
        "**podcast**\n",
        "\n",
        "- PK: id\n",
        " - The unique podcast id found in the huggingface data (i,e., TRdL6ZzWBS0  is the ID for Jed Buchwald: Isaac Newton and the Philosophy of Science | Lex Fridman Podcast #214)\n",
        "- title\n",
        " - The title of podcast (ie., Jed Buchwald: Isaac Newton and the Philosophy of Science | Lex Fridman Podcast #214)\n",
        "\n",
        "**segment**\n",
        "\n",
        "- PK: id\n",
        " - the unique identifier for the podcast segment. This was created by concatenating the podcast idx and the segment index together (ie., \"0;1\") is the 0th podcast and the 1st segment\n",
        "This is present in the as the \"custom_id\" field in the `embedding.jsonl` and batch_request.jsonl files\n",
        "- start_time\n",
        " - The start timestamp of the segment\n",
        "- end_time\n",
        " - The end timestamp of the segment\n",
        "- content\n",
        " - The raw text transcription of the podcast\n",
        "- embedding\n",
        " - the 128 dimensional vector representation of the text\n",
        "- FK: podcast_id\n",
        " - foreign key to podcast.id"
      ],
      "metadata": {
        "id": "7Y2HkhMZmHFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample document:\n",
        "# {\n",
        "#   \"custom_id\": \"89:115\",\n",
        "#   \"url\": \"/v1/embeddings\",\n",
        "#   \"method\": \"POST\",\n",
        "#   \"body\": {\n",
        "#     \"input\": \" have been possible without these approaches?\",\n",
        "#     \"model\": \"text-embedding-3-large\",\n",
        "#     \"dimensions\": 128,\n",
        "#     \"metadata\": {\n",
        "#       \"title\": \"Podcast: Boris Sofman: Waymo, Cozmo, Self-Driving Cars, and the Future of Robotics | Lex Fridman Podcast #241\",\n",
        "#       \"podcast_id\": \"U_AREIyd0Fc\",\n",
        "#       \"start_time\": 484.52,\n",
        "#       \"stop_time\": 487.08\n",
        "#     }\n",
        "#   }\n",
        "# }\n",
        "\n",
        "# Sample embedding:\n",
        "# {\n",
        "#   \"id\": \"batch_req_QZBmHS7FBiVABxcsGiDx2THJ\",\n",
        "#   \"custom_id\": \"89:115\",\n",
        "#   \"response\": {\n",
        "#     \"status_code\": 200,\n",
        "#     \"request_id\": \"7a55eba082c70aca9e7872d2b694f095\",\n",
        "#     \"body\": {\n",
        "#       \"object\": \"list\",\n",
        "#       \"data\": [\n",
        "#         {\n",
        "#           \"object\": \"embedding\",\n",
        "#           \"index\": 0,\n",
        "#           \"embedding\": [\n",
        "#             0.0035960325,\n",
        "#             126 more lines....\n",
        "#             -0.093248844\n",
        "#           ]\n",
        "#         }\n",
        "#       ],\n",
        "#       \"model\": \"text-embedding-3-large\",\n",
        "#       \"usage\": {\n",
        "#         \"prompt_tokens\": 7,\n",
        "#         \"total_tokens\": 7\n",
        "#       }\n",
        "#     }\n",
        "#   },\n",
        "#   \"error\": null\n",
        "# }"
      ],
      "metadata": {
        "id": "3EZuFdc9m9uP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create table statements that you'll write\n",
        "#TODO\n",
        "\n",
        "\n",
        "# may need to run this to enable vector data type if you didn't select AI in service\n",
        "# CREATE_EXTENSION = \"CREATE EXTENSION vector\"\n",
        "\n",
        "# TODO: Add create table statement\n",
        "CREATE_PODCAST_TABLE = \"\"\"\n",
        "CREATE TABLE IF NOT EXISTS podcasts (\n",
        "    podcast_id VARCHAR(255) PRIMARY KEY,\n",
        "    title TEXT,\n",
        "    last_updated TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n",
        ");\n",
        "\"\"\"\n",
        "\n",
        "CREATE_SEGMENT_TABLE = \"\"\"\n",
        "CREATE TABLE IF NOT EXISTS segments (\n",
        "    segment_id SERIAL PRIMARY KEY,\n",
        "    custom_id VARCHAR(255) UNIQUE NOT NULL,\n",
        "    podcast_id VARCHAR(255) REFERENCES podcasts(podcast_id) ON DELETE CASCADE,\n",
        "    input_text TEXT,\n",
        "    embedding_model VARCHAR(255),\n",
        "    dimensions INTEGER,\n",
        "    start_time FLOAT,\n",
        "    stop_time FLOAT,\n",
        "    embedding VECTOR(128),\n",
        "    response_status_code INTEGER,\n",
        "    response_request_id VARCHAR(255),\n",
        "    usage_prompt_tokens INTEGER,\n",
        "    usage_total_tokens INTEGER,\n",
        "    embedding_object_type VARCHAR(50),\n",
        "    embedding_data_object_type VARCHAR(50),\n",
        "    embedding_data_index INTEGER,\n",
        "    error_message TEXT,\n",
        "    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n",
        "    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n",
        ");\n",
        "\"\"\"\n",
        "\n",
        "conn = psycopg2.connect(CONNECTION)\n",
        "# TODO: Create tables with psycopg2 (example: https://www.geeksforgeeks.org/executing-sql-query-with-psycopg2-in-python/)\n",
        "\n",
        "\n",
        "conn.commit()\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "bU6fFAwb5EYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Extract needed data out of JSONL files. This may be the hard part!\n",
        "\n",
        "# TODO: What data do we need?\n",
        "# TODO: What data is in the documents jsonl files?\n",
        "# TODO: What data is in the embedding jsonl files?\n",
        "# TODO: Get some pandas data frames for our two tables so we can copy the data in!\n",
        "\n"
      ],
      "metadata": {
        "id": "v81052OY5BKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Optional #####\n",
        "# In addition to the embedding and document files you might like to load\n",
        "# the full podcast raw data via the hugging face datasets library\n",
        "\n",
        "# from datasets import load_dataset\n",
        "# ds = load_dataset(\"Whispering-GPT/lex-fridman-podcast\")\n"
      ],
      "metadata": {
        "id": "xo3Y8IHYRruE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO Copy all the \"podcast\" data into the podcast postgres table!\n",
        "\n",
        "import pandas as pd\n",
        "import io\n",
        "import psycopg2\n",
        "import json\n",
        "import os # Import the os module for file system operations\n",
        "import gc # Import garbage collector interface\n",
        "from pathlib import Path # Using pathlib for easier path manipulation\n",
        "from typing import List, Dict, Any, Tuple\n",
        "\n",
        "# --- Create Tables ---\n",
        "conn_outer = None # Define conn_outer outside try block for broader scope\n",
        "try:\n",
        "    conn_outer = psycopg2.connect(CONNECTION)\n",
        "    with conn_outer.cursor() as cur:\n",
        "        # Enable vector extension if not already enabled (run once as superuser or db owner if needed)\n",
        "        # print(\"Attempting to create vector extension if it doesn't exist...\")\n",
        "        # cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n",
        "        # print(\"Vector extension checked/created.\")\n",
        "\n",
        "        print(\"Attempting to create vector extension if it doesn't exist...\")\n",
        "        cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\") # <--- THIS LINE\n",
        "        print(\"Vector extension checked/created.\")\n",
        "\n",
        "        print(\"Creating podcasts table...\")\n",
        "        cur.execute(CREATE_PODCAST_TABLE)\n",
        "        print(\"Podcasts table created or already exists.\")\n",
        "\n",
        "        print(\"Creating segments table...\")\n",
        "        cur.execute(CREATE_SEGMENT_TABLE)\n",
        "        print(\"Segments table created or already exists.\")\n",
        "    conn_outer.commit()\n",
        "    print(\"Tables committed successfully.\")\n",
        "except psycopg2.Error as e:\n",
        "    print(f\"Database error during table creation: {e}\")\n",
        "    if conn_outer:\n",
        "        conn_outer.rollback()\n",
        "except Exception as e:\n",
        "    print(f\"A general error occurred: {e}\")\n",
        "finally:\n",
        "    if conn_outer:\n",
        "        conn_outer.close()\n",
        "        print(\"Database connection closed.\")\n",
        "\n",
        "\n",
        "# --- Data Extraction and Loading ---\n",
        "\n",
        "# Define BASE paths to the directories containing the files\n",
        "# IMPORTANT: Replace these with the actual paths to your 'documents' and 'embedding' parent directories\n",
        "BASE_DOCUMENTS_DIR = Path(\"documents/documents\") # e.g., ./data/documents\n",
        "BASE_EMBEDDINGS_DIR = Path(\"embedding/embedding\") # e.g., ./data/embedding\n",
        "\n",
        "def load_jsonl(file_path: Path) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Loads a JSONL file from a Path object into a list of dictionaries.\"\"\"\n",
        "    data = []\n",
        "    if not file_path.is_file():\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "        return []\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                try:\n",
        "                    data.append(json.loads(line))\n",
        "                except json.JSONDecodeError as e:\n",
        "                    print(f\"Skipping line {i+1} due to JSONDecodeError in {file_path.name}: {e} - Line: {line.strip()}\")\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while reading {file_path}: {e}\")\n",
        "        return []\n",
        "\n",
        "# --- vvv THIS IS THE FUNCTION TO REPLACE vvv ---\n",
        "# Delete the old find_file_pairs function definition (the one matching by name)\n",
        "# And insert this new definition:\n",
        "\n",
        "def find_file_pairs(doc_dir: Path, emb_dir: Path) -> List[Tuple[Path, Path]]:\n",
        "    \"\"\"\n",
        "    Finds pairs of document and embedding files by sorting the file lists\n",
        "    from each directory and pairing them element-wise.\n",
        "    Assumes both directories have the same number of files and the Nth\n",
        "    sorted file in docs corresponds to the Nth sorted file in embeddings.\n",
        "    \"\"\"\n",
        "    print(\"-\" * 20)\n",
        "    print(f\"Attempting to find files in Document Dir: {doc_dir.resolve()}\")\n",
        "    if not doc_dir.exists(): print(f\"ERROR: Document directory does not exist: {doc_dir}\")\n",
        "    if not os.access(str(doc_dir), os.R_OK): print(f\"ERROR: No read permissions for Document directory: {doc_dir}\")\n",
        "\n",
        "    print(f\"Attempting to find files in Embedding Dir: {emb_dir.resolve()}\")\n",
        "    if not emb_dir.exists(): print(f\"ERROR: Embedding directory does not exist: {emb_dir}\")\n",
        "    if not os.access(str(emb_dir), os.R_OK): print(f\"ERROR: No read permissions for Embedding directory: {emb_dir}\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    # Get all files directly within the specified directories\n",
        "    doc_files_list = [f for f in doc_dir.glob('*') if f.is_file()]\n",
        "    emb_files_list = [f for f in emb_dir.glob('*') if f.is_file()]\n",
        "\n",
        "    # Sort both lists alphabetically by filename\n",
        "    doc_files_list.sort(key=lambda x: x.name)\n",
        "    emb_files_list.sort(key=lambda x: x.name)\n",
        "\n",
        "    print(f\"\\nFound and sorted {len(doc_files_list)} files in {doc_dir}:\")\n",
        "    if not doc_files_list: print(\"  <None>\")\n",
        "    for f in doc_files_list: print(f\"  - {f.name}\")\n",
        "\n",
        "    print(f\"\\nFound and sorted {len(emb_files_list)} files in {emb_dir}:\")\n",
        "    if not emb_files_list: print(\"  <None>\")\n",
        "    for f in emb_files_list: print(f\"  - {f.name}\")\n",
        "\n",
        "    pairs = []\n",
        "    num_docs = len(doc_files_list)\n",
        "    num_embs = len(emb_files_list)\n",
        "\n",
        "    if num_docs == 0:\n",
        "        print(\"\\nERROR: No document files found.\")\n",
        "        return []\n",
        "    if num_embs == 0:\n",
        "        print(\"\\nERROR: No embedding files found.\")\n",
        "        return []\n",
        "\n",
        "    if num_docs != num_embs:\n",
        "        print(f\"\\nERROR: Mismatch in file counts! Found {num_docs} document files and {num_embs} embedding files. Cannot pair by order.\")\n",
        "        return [] # Stop processing\n",
        "\n",
        "    print(f\"\\nPairing {num_docs} files based on sorted order:\")\n",
        "    for i in range(num_docs):\n",
        "        doc_path = doc_files_list[i]\n",
        "        emb_path = emb_files_list[i]\n",
        "        print(f\"  Pair {i+1}: '{doc_path.name}' <--> '{emb_path.name}'\")\n",
        "        pairs.append((doc_path, emb_path))\n",
        "\n",
        "    print(\"-\" * 20)\n",
        "    return pairs\n",
        "\n",
        "# --- ^^^ END OF FUNCTION TO REPLACE ^^^ ---\n",
        "\n",
        "\n",
        "# --- Main Processing Loop ---\n",
        "\n",
        "# 1. Find all file pairs\n",
        "print(f\"Searching for file pairs in {BASE_DOCUMENTS_DIR} and {BASE_EMBEDDINGS_DIR}...\")\n",
        "# This line correctly calls the (now replaced) find_file_pairs function\n",
        "file_pairs = find_file_pairs(BASE_DOCUMENTS_DIR, BASE_EMBEDDINGS_DIR)\n",
        "total_pairs = len(file_pairs)\n",
        "\n",
        "if total_pairs == 0:\n",
        "    print(\"No document/embedding file pairs found or file counts mismatched. Exiting.\") # Updated message\n",
        "else:\n",
        "    print(f\"Found {total_pairs} file pairs to process.\")\n",
        "\n",
        "    # Loop through each pair\n",
        "    for i, (doc_file_path, emb_file_path) in enumerate(file_pairs):\n",
        "        print(f\"\\n--- Processing Pair {i+1}/{total_pairs}: {doc_file_path.name} ---\")\n",
        "\n",
        "        # 2. Load data for the current pair\n",
        "        print(f\"Loading documents from {doc_file_path.name}...\")\n",
        "        raw_documents_data = load_jsonl(doc_file_path)\n",
        "        if not raw_documents_data:\n",
        "            print(\"No document data loaded for this file. Skipping pair.\")\n",
        "            continue\n",
        "        # Parse docs into DataFrame\n",
        "        # ... (rest of parsing logic remains the same) ...\n",
        "        parsed_docs = []\n",
        "        for doc in raw_documents_data:\n",
        "            body = doc.get(\"body\", {})\n",
        "            metadata = body.get(\"metadata\", {})\n",
        "            parsed_docs.append({\n",
        "                \"custom_id\": doc.get(\"custom_id\"), \"doc_input_text\": body.get(\"input\"),\n",
        "                \"doc_embedding_model\": body.get(\"model\"), \"doc_dimensions\": body.get(\"dimensions\"),\n",
        "                \"doc_podcast_title\": metadata.get(\"title\"), \"doc_podcast_id\": metadata.get(\"podcast_id\"),\n",
        "                \"doc_start_time\": metadata.get(\"start_time\"), \"doc_stop_time\": metadata.get(\"stop_time\"),\n",
        "            })\n",
        "        df_docs = pd.DataFrame(parsed_docs)\n",
        "        print(f\"Loaded {len(df_docs)} documents.\")\n",
        "\n",
        "\n",
        "        print(f\"Loading embeddings from {emb_file_path.name}...\")\n",
        "        raw_embeddings_data = load_jsonl(emb_file_path)\n",
        "        if not raw_embeddings_data:\n",
        "            print(\"No embedding data loaded for this file. Skipping pair.\")\n",
        "            del df_docs; gc.collect()\n",
        "            continue\n",
        "        # Parse embeddings into DataFrame\n",
        "        # ... (rest of parsing logic remains the same) ...\n",
        "        parsed_embeddings = []\n",
        "        for emb in raw_embeddings_data:\n",
        "            response = emb.get(\"response\"); error_content = emb.get(\"error\")\n",
        "            entry = {\"custom_id\": emb.get(\"custom_id\")}\n",
        "            if response and isinstance(response.get(\"body\"), dict):\n",
        "                body = response.get(\"body\", {}); data_list = body.get(\"data\", [])\n",
        "                data_item = data_list[0] if data_list and isinstance(data_list, list) and len(data_list) > 0 else {}\n",
        "                usage = body.get(\"usage\", {})\n",
        "                entry.update({\n",
        "                    \"emb_response_status_code\": response.get(\"status_code\"), \"emb_response_request_id\": response.get(\"request_id\"),\n",
        "                    \"emb_embedding_object_type\": body.get(\"object\"), \"emb_embedding\": data_item.get(\"embedding\"),\n",
        "                    \"emb_embedding_data_object_type\": data_item.get(\"object\"), \"emb_embedding_data_index\": data_item.get(\"index\"),\n",
        "                    \"emb_embedding_model\": body.get(\"model\"), \"emb_usage_prompt_tokens\": usage.get(\"prompt_tokens\"),\n",
        "                    \"emb_usage_total_tokens\": usage.get(\"total_tokens\"),\n",
        "                    \"emb_error_message\": json.dumps(error_content) if error_content is not None else None,\n",
        "                })\n",
        "            else:\n",
        "                entry.update({\n",
        "                    \"emb_response_status_code\": response.get(\"status_code\") if response else None, \"emb_response_request_id\": None,\n",
        "                    \"emb_embedding_object_type\": None, \"emb_embedding\": None, \"emb_embedding_data_object_type\": None,\n",
        "                    \"emb_embedding_data_index\": None, \"emb_embedding_model\": None, \"emb_usage_prompt_tokens\": None,\n",
        "                    \"emb_usage_total_tokens\": None,\n",
        "                    \"emb_error_message\": json.dumps(error_content) if error_content is not None else \"Response or response body missing/invalid\",\n",
        "                })\n",
        "            parsed_embeddings.append(entry)\n",
        "        df_embeddings = pd.DataFrame(parsed_embeddings)\n",
        "        print(f\"Loaded {len(df_embeddings)} embeddings.\")\n",
        "\n",
        "\n",
        "        # 3. Prepare Podcast Data (for this pair)\n",
        "        # ... (logic remains the same) ...\n",
        "        print(\"Preparing podcast data for this pair...\")\n",
        "        if not df_docs.empty:\n",
        "            podcasts_df_chunk = df_docs[\n",
        "                [\"doc_podcast_id\", \"doc_podcast_title\"]\n",
        "            ].copy()\n",
        "            podcasts_df_chunk.rename(columns={\"doc_podcast_id\": \"podcast_id\", \"doc_podcast_title\": \"title\"}, inplace=True)\n",
        "            podcasts_df_chunk.drop_duplicates(subset=[\"podcast_id\"], inplace=True)\n",
        "            podcasts_df_chunk.dropna(subset=[\"podcast_id\"], inplace=True)\n",
        "            print(f\"Found {len(podcasts_df_chunk)} unique podcasts in this chunk.\")\n",
        "\n",
        "            # 4. Insert Podcast Data (using INSERT ON CONFLICT)\n",
        "            # ... (logic remains the same) ...\n",
        "            if not podcasts_df_chunk.empty:\n",
        "                conn_insert = None; inserted_count = 0; skipped_count = 0\n",
        "                try:\n",
        "                    conn_insert = psycopg2.connect(CONNECTION)\n",
        "                    with conn_insert.cursor() as cur:\n",
        "                        sql = \"INSERT INTO podcasts (podcast_id, title) VALUES (%s, %s) ON CONFLICT (podcast_id) DO NOTHING;\"\n",
        "                        data_tuples = [tuple(x) for x in podcasts_df_chunk.to_numpy()]\n",
        "                        cur.executemany(sql, data_tuples)\n",
        "                        inserted_count = cur.rowcount\n",
        "                        skipped_count = len(podcasts_df_chunk) - (inserted_count if inserted_count != -1 else 0)\n",
        "                        conn_insert.commit()\n",
        "                    print(f\"Podcast insert committed. Approx {inserted_count} inserted, {skipped_count} skipped (already existed).\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error inserting podcast data: {e}\")\n",
        "                    if conn_insert: conn_insert.rollback()\n",
        "                finally:\n",
        "                    if conn_insert: conn_insert.close()\n",
        "            else:\n",
        "                 print(\"No unique podcast data to insert for this pair.\")\n",
        "        else:\n",
        "            print(\"Skipping podcast preparation/insertion as document data was empty.\")\n",
        "\n",
        "\n",
        "        # 5. Prepare Segment Data (for this pair)\n",
        "        # ... (logic remains the same) ...\n",
        "        print(\"Preparing segment data for this pair...\")\n",
        "        if not df_docs.empty and not df_embeddings.empty:\n",
        "            df_merged = pd.merge(df_docs, df_embeddings, on=\"custom_id\", how=\"inner\")\n",
        "            if df_merged.empty:\n",
        "                 print(\"No matching segments found between documents and embeddings for this pair.\")\n",
        "            else:\n",
        "                segments_df_chunk = pd.DataFrame({\n",
        "                    \"custom_id\": df_merged[\"custom_id\"], \"podcast_id\": df_merged[\"doc_podcast_id\"],\n",
        "                    \"input_text\": df_merged[\"doc_input_text\"],\n",
        "                    \"embedding_model\": df_merged[\"doc_embedding_model\"].fillna(df_merged[\"emb_embedding_model\"]),\n",
        "                    \"dimensions\": df_merged[\"doc_dimensions\"], \"start_time\": df_merged[\"doc_start_time\"],\n",
        "                    \"stop_time\": df_merged[\"doc_stop_time\"], \"embedding\": df_merged[\"emb_embedding\"],\n",
        "                    \"response_status_code\": df_merged[\"emb_response_status_code\"],\n",
        "                    \"response_request_id\": df_merged[\"emb_response_request_id\"],\n",
        "                    \"usage_prompt_tokens\": df_merged[\"emb_usage_prompt_tokens\"],\n",
        "                    \"usage_total_tokens\": df_merged[\"emb_usage_total_tokens\"],\n",
        "                    \"embedding_object_type\": df_merged[\"emb_embedding_object_type\"],\n",
        "                    \"embedding_data_object_type\": df_merged[\"emb_embedding_data_object_type\"],\n",
        "                    \"embedding_data_index\": df_merged[\"emb_embedding_data_index\"],\n",
        "                    \"error_message\": df_merged[\"emb_error_message\"],\n",
        "                })\n",
        "                segments_df_chunk.dropna(subset=[\"custom_id\", \"podcast_id\"], inplace=True)\n",
        "                print(f\"Prepared {len(segments_df_chunk)} segment records for insertion.\")\n",
        "\n",
        "                # 6. Insert Segment Data (using fast_pg_insert)\n",
        "                # ... (logic remains the same) ...\n",
        "                if not segments_df_chunk.empty:\n",
        "                    segment_table_columns = [\n",
        "                        \"custom_id\", \"podcast_id\", \"input_text\", \"embedding_model\", \"dimensions\",\n",
        "                        \"start_time\", \"stop_time\", \"embedding\", \"response_status_code\",\n",
        "                        \"response_request_id\", \"usage_prompt_tokens\", \"usage_total_tokens\",\n",
        "                        \"embedding_object_type\", \"embedding_data_object_type\", \"embedding_data_index\",\n",
        "                        \"error_message\"\n",
        "                    ]\n",
        "                    segments_df_insert_chunk = segments_df_chunk[segment_table_columns]\n",
        "                    print(f\"Inserting {len(segments_df_insert_chunk)} segment records...\")\n",
        "                    try:\n",
        "                        # Ensure fast_pg_insert function is defined above\n",
        "                        fast_pg_insert(segments_df_insert_chunk, CONNECTION, \"segments\", segment_table_columns)\n",
        "                        print(f\"Segment insert successful for pair {i+1}.\") # Add success message\n",
        "                    except Exception as e:\n",
        "                        print(f\"Failed to insert segment data for pair {i+1}: {e}\")\n",
        "                else:\n",
        "                    print(\"No segment data to insert for this pair after processing.\")\n",
        "        else:\n",
        "            print(\"Skipping segment preparation/insertion as document or embedding data was missing/empty.\")\n",
        "\n",
        "\n",
        "        # 7. Clean up memory\n",
        "        # ... (logic remains the same) ...\n",
        "        print(\"Cleaning up DataFrames for this pair...\")\n",
        "        try:\n",
        "            del df_docs; del df_embeddings\n",
        "            if 'df_merged' in locals(): del df_merged\n",
        "            if 'podcasts_df_chunk' in locals(): del podcasts_df_chunk\n",
        "            if 'segments_df_chunk' in locals(): del segments_df_chunk\n",
        "            if 'segments_df_insert_chunk' in locals(): del segments_df_insert_chunk\n",
        "        except NameError: pass\n",
        "        gc.collect()\n",
        "        print(\"Cleanup complete.\")\n",
        "\n",
        "    print(\"\\n--- All file pairs processed ---\")\n"
      ],
      "metadata": {
        "id": "5W3f-2iTpGL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO Copy all the \"segment\" data into the segment postgres table!\n",
        "# HINT 1: use the recommender.utils.fast_pg_insert function to insert data into the database\n",
        "# otherwise inserting the 800k documents will take a very, very long time\n",
        "# HINT 2: if you don't want to use all your memory and crash\n",
        "# colab, you'll need to either send the data up in chunks\n",
        "# or write your own function for copying it up. Alternative to chunking maybe start\n",
        "# with writing it to a CSV and then copy it up?\n"
      ],
      "metadata": {
        "id": "ZTUsciGfpahF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## This script is used to query the database\n",
        "import os\n",
        "import psycopg2\n",
        "\n",
        "\n",
        "# Write your queries\n",
        "# Q1) What are the five most similar segments to segment \"267:476\"\n",
        "# Input: \"that if we were to meet alien life at some point\"\n",
        "# For each result return the podcast name, the segment id, segment raw text,  the start time, stop time, and embedding distance\n",
        "\n",
        "conn = psycopg2.connect(CONNECTION)\n",
        "cur = conn.cursor()\n",
        "cur.execute(\"\"\"\n",
        "    WITH target_segment AS (\n",
        "    SELECT embedding\n",
        "    FROM segments\n",
        "    WHERE custom_id = '267:476' -- Your target segment's custom_id\n",
        "    )\n",
        "\n",
        "    SELECT s.custom_id, input_text, podcast_id, start_time, stop_time, s.embedding <-> (SELECT embedding FROM target_segment) as distance\n",
        "    FROM segments s, target_segment\n",
        "    WHERE s.custom_id != '267:476'\n",
        "    ORDER BY distance ASC\n",
        "    LIMIT 5;\n",
        "\"\"\")\n",
        "for row in cur.fetchall():\n",
        "  print(row)\n",
        "\n",
        "conn.commit()\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "NvkG-51G5IDe",
        "outputId": "69353b6b-39e6-483d-a2e2-c5e9d9bf6136",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('113:2792', ' encounters, human beings, if we were to meet another alien', 'qLDp-aYnR1Y', 6725.62, 6729.86, 0.6483450674336982)\n",
            "('268:1019', ' Suppose we did meet an alien from outer space', '5f-JlzBuUUU', 2900.04, 2903.0800000000004, 0.6558106859320757)\n",
            "('305:3600', ' but if we think of alien civilizations out there', 'EwueqdgIvq4', 9479.960000000001, 9484.04, 0.6595433115268592)\n",
            "('18:464', ' So I think when we meet alien life from outer space,', 'kD5yc1LQrpQ', 1316.8600000000001, 1319.5800000000002, 0.6662026419636159)\n",
            "('71:989', ' because if aliens come to us', 'SFxIazwNP_0', 2342.34, 2343.6200000000003, 0.6742942635162208)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2) What are the five most dissimilar segments to segment \"267:476\"\n",
        "# Input: \"that if we were to meet alien life at some point\"\n",
        "# For each result return the podcast name, the segment id, segment raw text, the start time, stop time, and embedding distance\n",
        "\n",
        "conn = psycopg2.connect(CONNECTION)\n",
        "cur = conn.cursor()\n",
        "cur.execute(\"\"\"\n",
        "    WITH target_segment AS (\n",
        "    SELECT embedding\n",
        "    FROM segments\n",
        "    WHERE custom_id = '267:476' -- Your target segment's custom_id\n",
        "    )\n",
        "\n",
        "    SELECT s.custom_id, input_text, podcast_id, start_time, stop_time, s.embedding <-> (SELECT embedding FROM target_segment) as distance\n",
        "    FROM segments s, target_segment\n",
        "    WHERE s.custom_id != '267:476'\n",
        "    ORDER BY distance DESC\n",
        "    LIMIT 5;\n",
        "\"\"\")\n",
        "for row in cur.fetchall():\n",
        "  print(row)\n",
        "\n",
        "conn.commit()\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "Dq8ePSfrw8Ix",
        "outputId": "6b597ef4-13e9-4f0c-db29-c0b227e77965",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('119:218', ' a 73 Mustang Grande in gold?', 'd2bYwYxqJCM', 519.96, 523.8000000000001, 1.6157687685840119)\n",
            "('133:2006', ' for 94 car models.', '36_rM7wpN5A', 5818.62, 5820.82, 1.5863359073014982)\n",
            "('283:1488', ' when I called down to get the sauna.', 'uiNpESmPioQ', 3709.34, 3711.1000000000004, 1.572552805197421)\n",
            "('241:1436', ' which has all the courses pre-installed.', 'J6XcP4JOHmk', 4068.9, 4071.1400000000003, 1.5663319710412156)\n",
            "('307:3933', ' and very few are first class and some are budget.', 'rIpUf-Vy2JA', 10648.64, 10650.960000000001, 1.5616341289820461)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3) What are the five most similar segments to segment '48:511'\n",
        "\n",
        "# Input: \"Is it is there something especially interesting and profound to you in terms of our current deep learning neural network, artificial neural network approaches and the whatever we do understand about the biological neural network.\"\n",
        "# For each result return the podcast name, the segment id, segment raw text,  the start time, stop time, and embedding distance\n",
        "\n",
        "conn = psycopg2.connect(CONNECTION)\n",
        "cur = conn.cursor()\n",
        "cur.execute(\"\"\"\n",
        "    WITH target_segment AS (\n",
        "    SELECT embedding\n",
        "    FROM segments\n",
        "    WHERE custom_id = '48:511' -- Your target segment's custom_id\n",
        "    )\n",
        "\n",
        "    SELECT s.custom_id, input_text, podcast_id, start_time, stop_time, s.embedding <-> (SELECT embedding FROM target_segment) as distance\n",
        "    FROM segments s, target_segment\n",
        "    WHERE s.custom_id != '48:511'\n",
        "    ORDER BY distance ASC\n",
        "    LIMIT 5;\n",
        "\"\"\")\n",
        "for row in cur.fetchall():\n",
        "  print(row)\n",
        "\n",
        "conn.commit()\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "dmTK02bZk3pF",
        "outputId": "2d46c202-42ec-46a8-d09b-da9b2646db17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('155:648', ' Is there something interesting to you or fundamental to you about the circuitry of the brain', 'Ktj050DxG7Q', 3798.48, 3805.84, 0.652299685331962)\n",
            "('61:3707', ' of what we might discover about neural networks?', 'y3Umo_jd5AA', 8498.02, 8500.1, 0.7121050124628524)\n",
            "('48:512', \" And our brain is there. There's some there's quite a few differences. Are some of them to you either interesting or perhaps profound in terms of in terms of the gap we might want to try to close in trying to create a human level intelligence.\", '3t06ajvBtl0', 1846.84, 1865.84, 0.7195603322334674)\n",
            "('276:2642', ' Have these, I mean, small pockets of beautiful complexity. Does that, do cellular automata, do these kinds of emergence and complex systems give you some intuition or guide your understanding of machine learning systems and neural networks and so on?', 'SGzMElJ11Cc', 8628.16, 8646.16, 0.7357217735737499)\n",
            "('2:152', ' So is there something like that with physics where so deep learning neural networks have been around for a long time?', '-t1_ffaFXao', 610.86, 618.86, 0.7366969553372291)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4) What are the five most similar segments to segment '51:56'\n",
        "\n",
        "# Input: \"But what about like the fundamental physics of dark energy? Is there any understanding of what the heck it is?\"\n",
        "# For each result return the podcast name, the segment id, segment raw text,  the start time, stop time, and embedding distance\n",
        "\n",
        "conn = psycopg2.connect(CONNECTION)\n",
        "cur = conn.cursor()\n",
        "cur.execute(\"\"\"\n",
        "    WITH target_segment AS (\n",
        "    SELECT embedding\n",
        "    FROM segments\n",
        "    WHERE custom_id = '51:56' -- Your target segment's custom_id\n",
        "    )\n",
        "\n",
        "    SELECT s.custom_id, input_text, podcast_id, start_time, stop_time, s.embedding <-> (SELECT embedding FROM target_segment) as distance\n",
        "    FROM segments s, target_segment\n",
        "    WHERE s.custom_id != '51:56'\n",
        "    ORDER BY distance ASC\n",
        "    LIMIT 5;\n",
        "\"\"\")\n",
        "for row in cur.fetchall():\n",
        "  print(row)\n",
        "\n",
        "conn.commit()\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "jcfhAKKQk9rV",
        "outputId": "87892d07-4884-4643-bb34-ce473339a94d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('308:144', \" I mean, we don't understand dark energy, right?\", '_L3gNaAVjQ4', 500.44, 502.6, 0.6681965222094363)\n",
            "('243:273', \" Like, what's up with this dark matter and dark energy stuff?\", '6ePR2TWYVkI', 946.22, 950.12, 0.7355511762966292)\n",
            "('196:685', ' being like, what the hell is dark matter and dark energy?', '85F0FDsPHf8', 2591.72, 2595.9599999999996, 0.7631141596843518)\n",
            "('51:36', ' Do we have any understanding of what the heck that thing is?', 'WxfA1OSev4c', 216.0, 219.0, 0.7922019445543276)\n",
            "('122:831', ' That is a big question in physics right now.', 's78hvV3QLUE', 2374.9, 2377.6200000000003, 0.8022704628640559)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q5) For each of the following podcast segments, find the five most similar podcast episodes. Hint: You can do this by averaging over the embedding vectors within a podcast episode.\n",
        "\n",
        "#     a) Segment \"267:476\"\n",
        "\n",
        "#     b) Segment '48:511'\n",
        "\n",
        "#     c) Segment '51:56'\n",
        "\n",
        "# For each result return the Podcast title and the embedding distance\n",
        "\n",
        "conn = psycopg2.connect(CONNECTION)\n",
        "cur = conn.cursor()\n",
        "cur.execute(\"\"\"\n",
        "    WITH target_segment_data AS (\n",
        "      SELECT embedding AS target_embedding, podcast_id AS source_podcast_id\n",
        "      FROM segments\n",
        "      WHERE custom_id = '267:476'\n",
        "    ), average_podcast_embeddings AS (\n",
        "      SELECT s.podcast_id, title AS podcast_title, AVG(s.embedding) AS avg_episode_embedding\n",
        "      FROM segments s\n",
        "      JOIN podcasts p\n",
        "      ON s.podcast_id = p.podcast_id\n",
        "      GROUP BY s.podcast_id, title\n",
        "    )\n",
        "\n",
        "    SELECT ape.podcast_id, ape.podcast_title, ape.avg_episode_embedding <-> (SELECT target_embedding FROM target_segment_data) AS distance\n",
        "    FROM average_podcast_embeddings ape, target_segment_data tsd\n",
        "    WHERE ape.podcast_id != (SELECT source_podcast_id FROM target_segment_data)\n",
        "    ORDER BY distance ASC\n",
        "    LIMIT 5;\n",
        "\"\"\")\n",
        "for row in cur.fetchall():\n",
        "  print(row)\n",
        "\n",
        "print()\n",
        "cur.execute(\"\"\"\n",
        "  WITH target_segment_data AS (\n",
        "    SELECT embedding AS target_embedding, podcast_id AS source_podcast_id\n",
        "    FROM segments\n",
        "    WHERE custom_id = '48:511'\n",
        "  ), average_podcast_embeddings AS (\n",
        "    SELECT s.podcast_id, title AS podcast_title, AVG(s.embedding) AS avg_episode_embedding\n",
        "    FROM segments s\n",
        "    JOIN podcasts p\n",
        "    ON s.podcast_id = p.podcast_id\n",
        "    GROUP BY s.podcast_id, title\n",
        "  )\n",
        "\n",
        "  SELECT ape.podcast_id, ape.podcast_title, ape.avg_episode_embedding <-> (SELECT target_embedding FROM target_segment_data) AS distance\n",
        "  FROM average_podcast_embeddings ape, target_segment_data tsd\n",
        "  WHERE ape.podcast_id != (SELECT source_podcast_id FROM target_segment_data)\n",
        "  ORDER BY distance ASC\n",
        "  LIMIT 5;\n",
        "\"\"\")\n",
        "for row in cur.fetchall():\n",
        "  print(row)\n",
        "\n",
        "print()\n",
        "cur.execute(\"\"\"\n",
        "  WITH target_segment_data AS (\n",
        "    SELECT embedding AS target_embedding, podcast_id AS source_podcast_id\n",
        "    FROM segments\n",
        "    WHERE custom_id = '51:56'\n",
        "  ), average_podcast_embeddings AS (\n",
        "    SELECT s.podcast_id, title AS podcast_title, AVG(s.embedding) AS avg_episode_embedding\n",
        "    FROM segments s\n",
        "    JOIN podcasts p\n",
        "    ON s.podcast_id = p.podcast_id\n",
        "    GROUP BY s.podcast_id, title\n",
        "  )\n",
        "\n",
        "  SELECT ape.podcast_id, ape.podcast_title, ape.avg_episode_embedding <-> (SELECT target_embedding FROM target_segment_data) AS distance\n",
        "  FROM average_podcast_embeddings ape, target_segment_data tsd\n",
        "  WHERE ape.podcast_id != (SELECT source_podcast_id FROM target_segment_data)\n",
        "  ORDER BY distance ASC\n",
        "  LIMIT 5;\n",
        "\"\"\")\n",
        "for row in cur.fetchall():\n",
        "  print(row)\n",
        "\n",
        "conn.commit()\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "OT4yTTn4k_iX",
        "outputId": "053685fc-0ea5-4a1f-8ff7-7db5f208f6d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('-tDQ74I3Ovs', 'Podcast: Sara Walker: The Origin of Life on Earth and Alien Worlds | Lex Fridman Podcast #198', 0.7828978136062058)\n",
            "('50r-5ULcWgY', 'Podcast: Martin Rees: Black Holes, Alien Life, Dark Matter, and the Big Bang | Lex Fridman Podcast #305', 0.7879499391348677)\n",
            "('Gi8LUnhP5yU', 'Podcast: Max Tegmark: Life 3.0 | Lex Fridman Podcast #1', 0.7886899314049058)\n",
            "('l-NJrvyRo0c', 'Podcast: Sean Carroll: The Nature of the Universe, Life, and Intelligence | Lex Fridman Podcast #26', 0.7890653704600481)\n",
            "('rfKiTGj-zeQ', 'Podcast: Nick Bostrom: Simulation and Superintelligence | Lex Fridman Podcast #83', 0.7911210354871258)\n",
            "\n",
            "('piHkfmeU7Wo', 'Podcast: Christof Koch: Consciousness | Lex Fridman Podcast #2', 0.7537802160985114)\n",
            "('tg_m_LxxRwM', 'Podcast: Dileep George: Brain-Inspired AI | Lex Fridman Podcast #115', 0.7605152893560989)\n",
            "('aSyZvBrPAyk', 'Podcast: Tomaso Poggio: Brains, Minds, and Machines | Lex Fridman Podcast #13', 0.7615547981858913)\n",
            "('smK9dgdTl40', 'Podcast: Elon Musk: Neuralink, AI, Autopilot, and the Pale Blue Dot | Lex Fridman Podcast #49', 0.7761520759503151)\n",
            "('BCdV6BMMpOo', 'Podcast: Philip Goff: Consciousness, Panpsychism, and the Philosophy of Mind | Lex Fridman Podcast #261', 0.7872055032874042)\n",
            "\n",
            "('iNqqOLscOBY', 'Podcast: Sean Carroll: Quantum Mechanics and the Many-Worlds Interpretation | Lex Fridman Podcast #47', 0.7767144344304333)\n",
            "('-t1_ffaFXao', 'Podcast: Stephen Wolfram: Fundamental Theory of Physics, Life, and the Universe | Lex Fridman Podcast #124', 0.8080714284866961)\n",
            "('reYdQYZ9Rj4', 'Podcast: Donald Hoffman: Reality is an Illusion - How Evolution Hid the Truth | Lex Fridman Podcast #293', 0.8165829480979995)\n",
            "('j4_VyRDOmN4', 'Podcast: Cumrun Vafa: String Theory | Lex Fridman Podcast #204', 0.8173474448880219)\n",
            "('plcc6E-E1uU', 'Podcast: Avi Loeb: Aliens, Black Holes, and the Mystery of the Oumuamua | Lex Fridman Podcast #154', 0.8254520536023432)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q6) For podcast episode id = VeH7qKZr0WI, find the five most similar podcast episodes. Hint: you can do a similar averaging procedure as Q5\n",
        "\n",
        "# Input Episode: \"Balaji Srinivasan: How to Fix Government, Twitter, Science, and the FDA | Lex Fridman Podcast #331\"\n",
        "# For each result return the Podcast title and the embedding distance\n",
        "\n",
        "conn = psycopg2.connect(CONNECTION)\n",
        "cur = conn.cursor()\n",
        "cur.execute(\"\"\"\n",
        "WITH average_podcast_embeddings AS (\n",
        "        SELECT s.podcast_id, title AS podcast_title, AVG(s.embedding) AS avg_episode_embedding\n",
        "        FROM segments s\n",
        "        JOIN podcasts p\n",
        "        ON s.podcast_id = p.podcast_id\n",
        "        GROUP BY s.podcast_id, title\n",
        "    ), target_episode_data AS (\n",
        "        SELECT avg_episode_embedding\n",
        "        FROM average_podcast_embeddings\n",
        "        WHERE podcast_id = 'VeH7qKZr0WI'\n",
        "    )\n",
        "\n",
        "    SELECT ape.podcast_id, ape.podcast_title, ape.avg_episode_embedding <-> (SELECT avg_episode_embedding FROM target_episode_data) AS distance\n",
        "    FROM average_podcast_embeddings ape, target_episode_data ted\n",
        "    WHERE ape.podcast_id != 'VeH7qKZr0WI'\n",
        "    ORDER BY distance ASC\n",
        "    LIMIT 5;\n",
        "\"\"\")\n",
        "for row in cur.fetchall():\n",
        "  print(row)\n",
        "\n",
        "conn.commit()\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "_oKIVjn4lBYD",
        "outputId": "4d0c9edd-5411-4633-8dd5-38166639fd5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('7Grseeycor4', 'Podcast: Tyler Cowen: Economic Growth & the Fight Against Conformity & Mediocrity | Lex Fridman Podcast #174', 0.11950103776872197)\n",
            "('ifX_JnBfxTY', 'Podcast: Eric Weinstein: Difficult Conversations, Freedom of Speech, and Physics | Lex Fridman Podcast #163', 0.1257139025632404)\n",
            "('Pl3x4GINtBQ', 'Podcast: Michael Malice and Yaron Brook: Ayn Rand, Human Nature, and Anarchy | Lex Fridman Podcast #178', 0.12842690324343972)\n",
            "('1XGiTDWfdpM', 'Podcast: Steve Keen: Marxism, Capitalism, and Economics | Lex Fridman Podcast #303', 0.12916269225753493)\n",
            "('uykM3NhJbso', 'Podcast: Michael Malice: The White Pill, Freedom, Hope, and Happiness Amidst Chaos | Lex Fridman Podcast #150', 0.13040864953585687)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deliverables\n",
        "You will turn in a ZIP or PDF file containing all your code and a PDF file with the queries and results for questions 1-7."
      ],
      "metadata": {
        "id": "WBZVtZP4lDO2"
      }
    }
  ]
}